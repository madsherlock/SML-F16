\section{Discussion}
The \textbf{LOO} (Leave-one-out) configuration
yields much lower accuracy than the \textbf{MIX} configuration.
This is to be expected, as the testing data is drawn from different individual datasets
than the training data. Above 50 \% mean accuracy was shown at \(k=1\),
which might seem acceptable for certain applications.
However, as the example confusion matrix in figure \ref{fig:confmatknn-multiLOO} shows,
the classification of the \textbf{LOO} configuration is inappropriate
for general digit recognition, as nines are mostly classified as fours,
and most predicted nines are actually ones!

It may be possible to reduce this effect by increasing the number
of individuals represented in the training set, as this would introduce more
variation and possible closer, correct, neighbours for the currently
misclassified digits.
Combining this with increasing k could increase general applicability of the classifier,
although possible reducing the mean classification accuracy,
as figure \ref{fig:knn-acc-multi} shows.



The graphs seen in \ref{fig:knnFit_80}, \ref{fig:knnFit_90}, \ref{fig:knnFit_99} 
shows some weird output around k = 3, which unknown at the moment, but could be
due to an inaccurate data representation.  The perfomance of the system with normalization 
were not possible to compute due to timing constraint. It is at moment unclear whether
 the normalization would have any effect at all, the data consist of pixel values extracted from the 
hand writtens digits, those values are already constrained to a certain range,
 so it seems unclear at moment how an normalization would benefit.   