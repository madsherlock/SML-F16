\section{Methods}
\subsection{Preprocessing}
\label{sec:preprocessing}
Each student of the 2016 SML class filled out several sheets of paper
with digits from 0 to 9. An example of this data
is shown in figure 
\ref{fig:handwriten_digits}. 
This section describes the preprocessing scheme
applied to this data before applying the machine learning algorithms.
Each of these sheets of paper was scanned by the student at a resolution of 300 DPI.
\begin{figure}[ht]
\centering
\includegraphics[width = 0.5\textwidth]{img/cropY2016G2M1-100-7.png}
\caption[Handwritten digits]
{
Handwritten digits. Scanned image. Cropped, rotated, scaled, stretched
and downsampled to 100 DPI.
}
\label{fig:handwriten_digits}
\end{figure}

Each sheet held two 4x4" grids of 400 digits.
To ease the preprocessing task,
the student manually marked the pixel locations of the outer grid corners
before submitting the dataset to the class database.

The datasets were of varying quality and format:
some images were color images while others were not,
most images were stretched and skewed to some degree,
and many digits were overlapping the grid.

As the corner locations were assumed to be known,
the images were rotated, stretched and scaled
by the 4-point perspective transform \citep{perspective},
and then cropped,
to obtain ten images per student,
each being a 1200x1200 pixels image of 400 hand written digits.
This image was then downsampled to 200 DPI and to 100 DPI.
These preprocessing steps were done using MATLAB and ImageMagick.
After downsampling, the color images were converted to greyscale in R.

After color conversion, the images were smoothened in order
to make the digit data more uniform and easier to classify correctly.
It was chosen to apply a gaussian blur, as this smoothens
evenly in all directions.
The gaussian blur has one parameter named \(\sigma\),
and one which is the kernel size.
In this project, it was chosen to use the default
kernel size of \(2 \lceil3\sigma\rceil +1\),
and vary \(\sigma\) in order to find
optimal preprocessing parameters for the classification.

After smoothing the images of the grids,
each digit box (grid element) was extracted
as a vector of greyscale pixel intensities.
To avoid training the classifiers to recognize the parts
of the grid that are inside the digit boxes,
the digit boxes were cropped until the grid was no longer visible,
before vectorization.
A fixed cropping percentage of 10 \% along each border was used
(6 pixels border for 300 DPI, 4 pixels border for 200 DPI and 2 pixels border for 100 DPI).

Thus, at 300 DPI, a digit is represented in 2304 dimensions,
at 200 DPI, it is in 1024 dimensions,
and at 100 DPI, it is 256 dimensions.
Naturally, the computations involved in the classification
are faster when working with the lower-dimensional data.
As part of the \(\sigma\) optimization process,
it was therefore chosen to measure the k-NN prediction time
on the raw datasets at each pixel density,
giving an optimized \(\sigma\)-pixel density combination
for the digit recognition problem.

All above steps only need to be carried out once per
\(\sigma\)-pixel density combination.
The implemented R scripts thus compute this preprocessing
as an offline step, storing the preprocessed
datasets on disk.

\subsubsection{Principal Component Analysis}
In previous work, it was found that including Principal Component Analysis (PCA)
as a final step of the preprocessing could speed up the training
at an acceptable cost of reduced accuracy.

PCA is a statistical tool used for dimensionality reduction,
and works by creating new variables from the input variables.
These new variables, called principal components,
are linear combinations of the input variables,
and are ordered by how much of the input data variance they explain.
The statistical optimality of PCA is only achieved
when the input data is linear. This is not the case
for hand written digits. This may explain part
of the loss of accuracy.

As the main purpose of this project is the comparison of two algorithms,
it was chosen to apply PCA to the data before machine learning.
In addition, the PCA was used to verify the previous
steps of the preprocessing scheme qualitatively,
ensuring that the main data variance is not due to the grid lines of the input data.

The dimensionality reduction lies in transforming the input
data by a matrix representing the principal components
(the eigenvectors), and then using only the
first N components, enough to explain a given percentage of the cumulated variance.

The data transformation matrix obtained from one PCA
can be stored and applied to new data.
It was therefore chosen to include the PCA in the training of the classifiers,
storing the information necessary to transform new data
with the trained classifiers.
PCA is performed before training, on the training set,
and the number of components is chosen dynamically as the minimum
required to explain a given percentage of the cumulated variance
of the training set.
Thus, the tuning parameter investigated in this work,
is the percentage of the cumulated variance to explain,
making the PCA an online step of the preprocessing scheme.
The results of PCA are therefore not stored on disk,
except when saved along with trained classifiers.

The percentage is chosen using the Elbow Method.

\subsection{Datasets, resampling methods and classifier evaluation}
The datasets used in this project are described in this section,
along with the resampling schemes employed for reporting
overall classification accuracies.
The combinations of datasets and resampling schemes makes
for the classifier evaluation schemes used in this project,
also described in this section.

All the data used in this project is contained in the dataset
denoted by \(D_{ALL}\), consisting of data from the 23
individuals who submitted their datasets.
Each of these individual datasets is denoted by the group number X
and group member index Y as \(GXMY\).
Ten individual datasets were selected from \(D_{ALL}\),
yielding a smaller dataset
\(D_{FEWER}=G1M1\cup G2M1\cup G3M1\cup G4M1\cup G5M1\cup G6M1\cup G7M1\cup G8M1\cup G11M1\cup G13M1\).

10-fold cross validation applied to a dataset containing data from more than
one individual is denoted the \textbf{MIX} method.
Here, data from each individual has a very high probability of
being represented in both the training set and the test set.

A method denoted by \textbf{LOO} (Leave-One-person-Out) was applied to datasets
containing data from \(n>1\) individuals by dividing
the data into \(n\) folds. Each fold contains data from \(n-1\)
individuals for training. The test data of each fold is the
data of the person not represented in the training set.

Four classifier evaluation schemes are defined:
\begin{itemize}
\item One Person:
Data from only one person is used,
and reported classification accuracy is found by 10-fold cross validation.
\item Single Persons:
One Person, applied to every persons dataset. The reported
classification accuracy is the mean of the 23 accuracies.
The reported standard deviation is found by pooling
the mean of the internal variances of each 10-fold cross validation
with the external variance of the 23 accuracies,
and taking the square root.
\item All Mixed:
\(D_{ALL}\), \textbf{MIX}.
\item All LOO:
\(D_{ALL}\), \textbf{LOO}.
\end{itemize}
The three schemes Single Persons, All Mixed and All LOO
were used for classifier comparison,
while One Person was used during preprocessing
scheme development.

The hardest scheme, All LOO, is the one used for
reporting general classifier accuracy.

\subsection{k-Nearest Neighbours}
\label{sec:knn}
The following introduction to k-Nearest Neighbours (k-NN) is based on \citep{mlwr}.
k-NN
is a supervised learning algorithm.
The method is non-parametric,
meaning that it does not make any assumption about the distribution of the input data.
The method is referred to as a lazy learning algorithm,
as it does not use the training data to do any generalization (abstraction).
Figure \ref{fig:knn-example} \citep{knnwiki} shows the basic principle of the algorithm.
\begin{figure}[ht]
\centering
\includegraphics[width = 0.3\textwidth]{img/kNN-classification.png}
\caption[k-NN classification example]{
k-NN classification example, 2-dimensional 2-class data.
The test sample, the green circle,
has to be classified as one of the two classes indicated
as red triangles and blue squares, based on the number
of training samples close to the test samples.
With k=3 (the inner circle), the sample would be classified as belonging
to the class of the red triangles.
With k=5 (the outer dashed circle), the sample would be classified as belonging
to the class of the blue squares.
Author: Antti Ajanki \citep{knnwiki}.
}
\label{fig:knn-example}
\end{figure}

The purpose of the algorithm is to classify a
new \(n\)-dimensional test sample based on its \(n\) attributes
and the training samples that have been used.
The algorithm decides which class a new sample belongs to by
inspecting the k nearest neighbours,
and based on the class which is represented most strongly in the vicinity
of the test sample,
the class of the new sample is determined.
k can be both an even or an odd number.
In the case of a tie with a even numbered k,
the class of the test sample is randomly chosen among the classes holding
the majority of the votes.

The k-NN classifier performs differently with different k,
meaning k is a tuning parameter which can be used to optimize the classifier
to certain data.

Several metrics for measuring the distance between
two samples \(\mathbf{w}\) and \(\mathbf{v}\) can be employed,
including Euclidean distance, eq. \eqref{eq:euclidean-distance},
and Manhattan distance, eq. \eqref{eq:manhattan-distance}.
\begin{equation}
d(\mathbf{w},\mathbf{v}) = \sqrt{\sum_{i=1}^n (w_i - v_i)^2}
\label{eq:euclidean-distance}
\end{equation}
\begin{equation}
d(\mathbf{w},\mathbf{v}) = \sum_{i=1}^n \left|w_i - v_i\right|
\label{eq:manhattan-distance}
\end{equation}
The Euclidean distance metric is used in this project.

The advantage of using k-NN is that it is simple to use,
and works well on basic problems.
However, it can be slow for real-time prediction applications,
especially if the dataset used for training is large,
as the method is a lazy learner.
Another disadvantage of the method is that it does not
learn anything from the training data, but just stores it;
No particular new insight is gained.
 
\subsection{Support Vector Machines}
\label{sec:svm}
Support Vector Machines (SVM) is a supervised learning
method that analyzes training data 
and tries to recognize patterns within the classification domain.
The standard SVM method is a binary classifier,
which predicts which of the two classes a given input belongs two. 
Prediction is done based on the model built from the training session.
The model provides a constraining border which makes the distinction
between the two classes.
The following introduction to SVM is based on \citep{svmwiki} and \citep{mlwr}.

For \(n\)-dimensional data,
SVM constructs an \(n\)-dimensional hyperplane or a set of hyperplanes.
The hyperplane with the largest distance to the nearest training sample
of any class (so-called functional margin), provides the best separation,
since it results in the clearest distinction between the classes,
and lower general error,
as illustrated by figure \ref{fig::SVM-illustrated} \citep{svmmathworks}.

The goal in SVM is to find this hyperplane which provides the optimal separation 
of the classes by having the widest margin. 

\begin{figure}[ht]
\centering
%\includegraphics[width = 0.5\textwidth]{img/SVM-illu.png} %no source, Kiddi!
\includegraphics[width = 0.5\textwidth]{img/svmhyperplane}
\caption[SVM Illustrated]{SVM Illustrated. Source: \citep{svmmathworks}.}
\label{fig::SVM-illustrated}
\end{figure}

The training data for this method consists of a set of input vectors denoted as 
$\mathbf{x_i}$.
Each input vector has a number of component features. Each input 
vector is given a label, indicating its class.

The hyperplane is given by eq. \eqref{eq:hyperplane}.
\begin{equation}
\mathbf{w} \cdot \mathbf{x} + b = 0
\label{eq:hyperplane}
\end{equation}
$\mathbf{w}$ determines the orientation of the plane, and 
$\mathbf{b}$ is the offset of the plane from the origin.
 
The seperaing hyperplane which maximizes the margin can be found by examining 
the convex hull of each class’s training data and then find the closest points 
in the two convex hulls. The convex hull of a set of points is the
smallest convex set containing the points. If the hyperplane that bisects both 
convex hulls can be found, the resulting classifier is robust with
respect to the training data.
This concept is illustrated in figure \ref{fig:convex_hull} \citep{convexhull}.
\begin{figure}[ht]
\centering
\includegraphics[width = 0.5\textwidth]{img/convex_hull.png}
\caption[Best plane bisects closest points in the convex hulls]
{Best plane bisects closest points in the convex hulls. The convex hulls 
are labeled c and d. Source: \citep{convexhull}.}
\label{fig:convex_hull}
\end{figure}  

To find the plane, the algorithm has to find the points closest
to the plane, which can 
be found by solving the quadratic programming problem of eq. \eqref{eq:quadratic-svm}.
\begin{equation}
min_\alpha~\left(\frac{1}{2} ||c-d||^2\right)
\label{eq:quadratic-svm}
\end{equation}
c and d are the closest to be found points,
defined by eqs. \eqref{eq:svm-c} and \eqref{eq:svm-d}.
\begin{equation}
\begin{aligned}
&c = \sum_{y_i~\in~class1} \alpha_ix_i  
& \text{subject to}
&& \sum_{y_i~\in~class1}\alpha_i =1 
&& \alpha_i \geq 0
\end{aligned}
\label{eq:svm-c} 
\end{equation}
\begin{equation}
\begin{aligned}
&d = \sum_{y_i~\in~class2} \alpha_ix_i  
& \text{subject to}
&& \sum_{y_i~\in~class2}\alpha_i =1 
&& \alpha_i \geq 0
\end{aligned} 
\label{eq:svm-d} 
\end{equation}
%Direct plagiarism:
%An alternative approach involves a search through the space of every possible 
%hyperplane in order to find a set of two parallel planes that divide the points 
%into homogeneous groups yet themselves are as far apart as possible.\\

In the case of non-linearly separable data,
a linear hyperplane cannot be used 
to define the solution. 
For this purpose is a slack variable introduced, which allows some points on the 
incorrect side of the margin, creating a soft margin, when a linear hyperplane 
was used to separate them.\\

A different approach for solving this problem would be using the kernel trick. 
The kernel trick involves transforming in $\mathbb{R}^n \rightarrow 
\mathbb{R}^{n+1}$. 
The challenging part is to find such transformation, $\phi$ which allow 
transforming data into a higher dimension in which it is separable. 

Kernel functions are in general in the following form:
\begin{equation}
K(\overrightarrow{x_i},\overrightarrow{x_j}) = \phi(\overrightarrow{x_i}) \cdot 
\phi(\overrightarrow{x_j}) 
\end{equation}

$x_i$ and $x_j$ illustrates two different feature vectors.

Previous work with the caret package of R
involved experimenting with the linear kernel of eq. \eqref{eq:linearkernel}
and the polynomial kernel of eq. \eqref{eq:polynomialkernel}
and showed promising results.
In particular, the polynomial kernel with a degree \(d=2\)
and a scaling factor \(s=0.1\) was found to yield generally good results.
\begin{equation}
K(\overrightarrow{x_i},\overrightarrow{x_j}) = (\overrightarrow{x_i}) \cdot 
(\overrightarrow{x_j})
\label{eq:linearkernel} 
\end{equation}
\begin{equation}
K(\overrightarrow{x_i},\overrightarrow{x_j}) = (s(\overrightarrow{x_i}) \cdot 
(\overrightarrow{x_j})+c)^d
\label{eq:polynomialkernel}
\end{equation}

The SVM algorithm used in this project is parametrized
by the cost of constraints violation \(C\),
a tuning parameter which is varied and chosen
according to classification accuracy.

\subsection{Parameter optimization}
The k-NN and SVM classifiers were each trained on the \(D_{FEWER}\)
dataset with the \textbf{LOO} method, as the dataset
is greatly varied, and the \textbf{LOO} method
represents the most difficult classification problem.

The classifiers were trained in order to find
suitable parameters \(k\) and \(C\) for the more computationally
expensive comparison tests.
Thus, the result of the parameter optimization is
only one value of \(k\), and one value of \(C\),
used for three classification problems encompassing \(D_{ALL}\).

\subsection{Classifier comparison}
The k-NN and SVM
classfiers were compared
after parameter optimization.
They were compared on overall classification accuracy
using the three classifier evaluation schemes
Single Persons, All Mixed and All LOO.
After finding the overall accuracy of each classifier,
a Z-test was applied to determine whether
the accuracies were significantly different.
This assumes the Central Limit Theorem holds,
as \(D_{ALL}\) holds 4000 digits per person (23 persons),
each one tested once per method.

As the k-NN classifier used in this project
has an \(O\left(d N\right)\) time complexity,
linear with the training set size,
it is interesting to investigate if this makes it slower than SVM
for the classifications of this project.
Therefore, the k-NN and SVM prediction (classification)
times per digit were found for the classifiers
trained with the All LOO scheme, on the \(G2M2\) dataset,
based on 30 repeated runs.

\subsection{Hand written digit dataset quality}
In order to be able to identify the individual with the worst
digit hand writing, an objective measure
of digit hand writing quality must be defined.
There are at least three definitions to consider:
\begin{enumerate}
\item[1] Good quality is proportional to classification accuracy when using all other
persons data for training, and the questionable quality dataset for testing.
\item[2] Good quality is proportional to classification accuracy when using all
persons data, including the dataset under test, in the training set.
\item[3] Good quality is proportional to One Person classification accuracy,
when using only the persons own data for training and for testing.
\end{enumerate}

Definition 1 is based on the idea that the majority wins.
However, it is possible that the entire class, except one person,
has really ugly and bad hand writing. In this case,
the one person with good hand writing would be identified
as the one with the worst digit hand writing.

Definition 2 suffers from the same majority bias,
albeit to a smaller degree. The biggest problem with this
definition is that the k-NN classifier would always find neighbours very
close to the test set, possibly giving much higher
accuracies than the SVM classifier.
Then it would be difficult to distinguish one persons
writing quality from anothers.

Definition 3 does not suffer from the majority bias.
While this gives a less democratic measure,
it is also possibly much more fair and objective.
Here, quality is proportional to two properties of the individual dataset:
within-digit consistency and between-digit variation.
While these properties may be obtained with an ugly hand writing,
they are the most objective measure of digit hand writing quality
available here, as they provide
a direct measure of dataset quality for \textit{classification},
a problem of high interest in this project.

Definition 3 is chosen as a measure of digit hand writing quality.
As such, a variant of the One Person
classifier evaluation scheme is used,
in which it is applied to every persons dataset,
and then compared.